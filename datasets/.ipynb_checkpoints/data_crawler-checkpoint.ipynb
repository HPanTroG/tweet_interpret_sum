{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tweepy\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import csv\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# student account\n",
    "consumer_key = 'uOLcWq6subGIyYVpc2tLsZhT8'\n",
    "consumer_key_secret = '9fcReWPgPa0Rk2gRndOGuPlRtyfHysNjrttqzysxhFdeh8H6pA'\n",
    "access_token = '1323575376669515776-zwb8gu1WHH69yW0Je0pIxB20oRwkN0'\n",
    "access_token_secret = 'E98yzbiL8v3JehoWlfLYRchTqu7rjPXg31q2nzwpRdXZs'\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_key_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# research account\n",
    "# consumer_key = 'Y2wLR0x8hp0dZt9wZuN9QZXa4'\n",
    "# consumer_key_secret = '9JjTy7VFdthVf0zilNiIuDF5M5NUckXC7a74W9oTu2i9EAdvxZ'\n",
    "# access_token = '1323575376669515776-8D35EbpdHORxqIHISGymFn1jGiOgvA'\n",
    "# access_token_secret = 'yuCHCv1ZPg4bOfQe73sg3jYWcOrRvK6TGPW3IH3WA4nEE'\n",
    "\n",
    "# auth = tweepy.OAuthHandler(consumer_key, consumer_key_secret)\n",
    "# auth.set_access_token(access_token, access_token_secret)\n",
    "# api = tweepy.API(auth,proxy=\"127.0.0.1:8083\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en\n",
      "The status 1266978261701210112 is posted by geeksforgeeks\n",
      "This status says : \n",
      "\n",
      "'Avoid errors, not client calls\\n.\\nGeeks, Keep this going...\\n.\\n#sundayvibes #programming #programmingmemes #coding https://t.co/JkA5iStofZ'\n",
      "\n",
      "en\n",
      "The status 1266735261012111360 is posted by geeksforgeeks\n",
      "This status says : \n",
      "\n",
      "'With the access to our Job Portal, find the jobs that are best for you &amp; experience happy placement journey....\\n.\\nL… https://t.co/mzMMFVzjMv'\n",
      "\n",
      "en\n",
      "The status 1266342841648898049 is posted by geeksforgeeks\n",
      "This status says : \n",
      "\n",
      "'My reaction to this Lockdown : \\n\\n\"Der Lagi Lekin... Maine Ab Hai Jeena Seekh Liya\" \\n\\nWhat\\'s your reaction to it?\\n.… https://t.co/nH9L0eewSr'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# list of status IDs to be fetched  \n",
    "id_ = [1266978261701210112, 1266735261012111360, 1266342841648898049] \n",
    "  \n",
    "# fetching the statuses \n",
    "statuses = api.statuses_lookup(id_) \n",
    "  \n",
    "# printing the statuses \n",
    "for status in statuses: \n",
    "    print(status.lang)\n",
    "    print(\"The status \" + str(status.id) + \" is posted by \" + status.user.screen_name) \n",
    "    print(\"This status says : \\n\\n\" + repr(status.text), end = \"\\n\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet fetchedRT @ADB_HQ: NEWS: ADB grants $300M loan to help Philippines shift to new basic education system http://t.co/3BVfLpWnBO #K12program\n"
     ]
    }
   ],
   "source": [
    "tweetFetched = api.get_status('544815534475595777')\n",
    "print(\"Tweet fetched\" + tweetFetched.text)\n",
    "text = tweetFetched.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all input files of tweet ids\n",
    "def read_files(folder):\n",
    "    files = []\n",
    "    for dir, _, filenames in os.walk(folder):\n",
    "        while len(filenames) > 0:\n",
    "            f = filenames.pop()\n",
    "            if f.endswith(\".csv\"):\n",
    "                files.append(os.path.join(dir, f))\n",
    "    files.sort()         \n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nguyen/tweet_interpret_sum/datasets/unlabeled_data/2014_pakistan_floods_en/tweet_ids/2014-09-pakistan_floods_20140906_vol-1.json.csv\n",
      "/home/nguyen/tweet_interpret_sum/datasets/unlabeled_data/2014_pakistan_floods_en/tweet_ids/2014-09-pakistan_floods_20140907_vol-1.json.csv\n",
      "/home/nguyen/tweet_interpret_sum/datasets/unlabeled_data/2014_pakistan_floods_en/tweet_ids/2014-09-pakistan_floods_20140908_vol-2.json.csv\n",
      "/home/nguyen/tweet_interpret_sum/datasets/unlabeled_data/2014_pakistan_floods_en/tweet_ids/2014-09-pakistan_floods_20140909_vol-3.json.csv\n",
      "/home/nguyen/tweet_interpret_sum/datasets/unlabeled_data/2014_pakistan_floods_en/tweet_ids/2014-09-pakistan_floods_20140910_vol-3.json.csv\n",
      "/home/nguyen/tweet_interpret_sum/datasets/unlabeled_data/2014_pakistan_floods_en/tweet_ids/2014-09-pakistan_floods_20140911_vol-3.json.csv\n",
      "/home/nguyen/tweet_interpret_sum/datasets/unlabeled_data/2014_pakistan_floods_en/tweet_ids/2014-09-pakistan_floods_20140911_vol-4.json.csv\n",
      "/home/nguyen/tweet_interpret_sum/datasets/unlabeled_data/2014_pakistan_floods_en/tweet_ids/2014-09-pakistan_floods_20140912_vol-4.json.csv\n",
      "/home/nguyen/tweet_interpret_sum/datasets/unlabeled_data/2014_pakistan_floods_en/tweet_ids/2014-09-pakistan_floods_20140913_vol-4.json.csv\n",
      "/home/nguyen/tweet_interpret_sum/datasets/unlabeled_data/2014_pakistan_floods_en/tweet_ids/2014-09-pakistan_floods_20140914_vol-5.json.csv\n",
      "/home/nguyen/tweet_interpret_sum/datasets/unlabeled_data/2014_pakistan_floods_en/tweet_ids/2014-09-pakistan_floods_20140915_vol-5.json.csv\n",
      "/home/nguyen/tweet_interpret_sum/datasets/unlabeled_data/2014_pakistan_floods_en/tweet_ids/2014-09-pakistan_floods_20140916_vol-6.json.csv\n",
      "/home/nguyen/tweet_interpret_sum/datasets/unlabeled_data/2014_pakistan_floods_en/tweet_ids/2014-09-pakistan_floods_20140921_vol-6.json.csv\n",
      "/home/nguyen/tweet_interpret_sum/datasets/unlabeled_data/2014_pakistan_floods_en/tweet_ids/2014-09-pakistan_floods_20140922_vol-6.json.csv\n",
      "/home/nguyen/tweet_interpret_sum/datasets/unlabeled_data/2014_pakistan_floods_en/tweet_ids/2014-09-pakistan_floods_20140924_vol-7.json.csv\n"
     ]
    }
   ],
   "source": [
    "# files = read_files('/home/nguyen/tweet_interpret_sum/datasets/unlabeled_data/2015_nepal_earthquake_en/tweet_ids/')\n",
    "files = read_files('/home/nguyen/tweet_interpret_sum/datasets/unlabeled_data/2014_pakistan_floods_en/tweet_ids/')\n",
    "# files = read_files('/home/nguyen/tweet_interpret_sum/datasets/unlabeled_data/2014_typhoon_hagupit_en/tweet_ids/')\n",
    "for file in files:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/nguyen/tweet_interpret_sum/datasets/unlabeled_data/2014_pakistan_floods_en/tweet_ids/2014-09-pakistan_floods_20140907_vol-1.json.csv'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawled_data = pd.read_csv('/home/nguyen/tweet_interpret_sum/datasets/unlabeled_data/2014_pakistan_floods_en/crawled_data/2014-09-pakistan_floods_20140906_vol-1.json.csv', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>508332152073625601</td>\n",
       "      <td>2014-09-06 19:13:24</td>\n",
       "      <td>'@asmashirazi Damage caused by FLOODS z 1000 f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>508332154271461376</td>\n",
       "      <td>2014-09-06 19:13:24</td>\n",
       "      <td>'RT @Assam_24X7: Assam govt is throwing money ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>508332173532073985</td>\n",
       "      <td>2014-09-06 19:13:29</td>\n",
       "      <td>Wait till bay get dressed I'm gone flood y'all...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>508332177608568832</td>\n",
       "      <td>2014-09-06 19:13:30</td>\n",
       "      <td>'@HannanSaleem My friends are stuck in the flo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>508332184613027840</td>\n",
       "      <td>2014-09-06 19:13:31</td>\n",
       "      <td>'Please take a little time and pray for Pakist...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id           created_at  \\\n",
       "0  508332152073625601  2014-09-06 19:13:24   \n",
       "1  508332154271461376  2014-09-06 19:13:24   \n",
       "2  508332173532073985  2014-09-06 19:13:29   \n",
       "3  508332177608568832  2014-09-06 19:13:30   \n",
       "4  508332184613027840  2014-09-06 19:13:31   \n",
       "\n",
       "                                                text  \n",
       "0  '@asmashirazi Damage caused by FLOODS z 1000 f...  \n",
       "1  'RT @Assam_24X7: Assam govt is throwing money ...  \n",
       "2  Wait till bay get dressed I'm gone flood y'all...  \n",
       "3  '@HannanSaleem My friends are stuck in the flo...  \n",
       "4  'Please take a little time and pray for Pakist...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crawled_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_tweet=str(crawled_data.loc[crawled_data.shape[0]-1]['tweet_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= pd.read_csv(files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tweet_id'] = data['tweet_id'].apply(lambda x: str(x[1:-1]))\n",
    "tweet_ids = list(data['tweet_id'])\n",
    "last_tweet_idx = tweet_ids.index(last_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23876\n",
      "23873\n"
     ]
    }
   ],
   "source": [
    "print(len(tweet_ids))\n",
    "print(last_tweet_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'508502502841409536'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_ids[last_tweet_idx+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'508502497741111296'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetFetched = api.get_status(tweet_ids[last_tweet_idx])\n",
    "creation_time = tweetFetched.created_at\n",
    "text = tweetFetched.text\n",
    "tweet_id = tweetFetched.id_str\n",
    "lan = tweetFetched.lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RT @AlZarraRr: Flood waters submerge large swathes of Occupied #Kashmir; 120 dead #KashmirFloods'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_crawler(corpusFile, output_file, start_tweet='', error_file ='', winSize = 3):\n",
    "\n",
    "    counter = 0\n",
    "    tweet_ids = []\n",
    "#     with open(output_file, \"w\") as f:\n",
    "#         f.write(\"tweet_id\\tcreated_at\\ttext\\n\")\n",
    "\n",
    "    data = pd.read_csv(corpusFile)\n",
    "\n",
    "    data['tweet_id'] = data['tweet_id'].apply(lambda x: str(x[1:-1]))\n",
    "    tweet_ids = list(data['tweet_id'])\n",
    "    \n",
    "    start_idx = 0\n",
    "    end_idx = len(tweet_ids)\n",
    "    \n",
    "    if start_tweet!=\"\":\n",
    "        start_idx = tweet_ids.index(start_tweet)+1\n",
    "    print(\"Start idx: {}, end idx: {}\".format(start_idx, end_idx))\n",
    "    \n",
    "    sleepTime = 3\n",
    "    trainingDataSet = []\n",
    "    i = 0\n",
    "    num_crled_tweets = 0\n",
    "    for idx in range(start_idx, end_idx):\n",
    "        print(\".................\")\n",
    "        ids_ = tweet_ids[idx: idx+winSize]\n",
    "        i+=winSize\n",
    "        \n",
    "        try:\n",
    "#             tweetFetched = api.get_status(int(tweet_id))\n",
    "            statuses = api.statuses_lookup(ids_) \n",
    "            for tweet in statuses:\n",
    "                if tweet.lang !=\"en\":\n",
    "                    continue\n",
    "                print(\"{}\\t{}\\t{}\\n\".format(tweet.id, tweet.created_at, repr(tweet.text)))\n",
    "                with open(\"test.csv\", \"a\") as f:\n",
    "                    f.write(\"{}\\t{}\\t{}\\n\".format(tweet.id, tweet.created_at, repr(tweet.text)))\n",
    "                \n",
    "            time.sleep(sleepTime)\n",
    "            num_crled_tweets +=1\n",
    "            if i%100 == 0:\n",
    "                print(r\"#ids:{},cwled:{},{},{}\".format(i, num_crled_tweets, tweet_id, text))\n",
    "            \n",
    "        except Exception as exp:\n",
    "            print(i, ids_, exp)\n",
    "#             with open(error_file, \"w\") as f:\n",
    "#                 f.write(\"{},{},{}\\n\".format(i, ids_, exp))\n",
    "        \n",
    "    print(\"#######\")\n",
    "    print(\"Number of tweet ids: \", i)\n",
    "    print(\"Number of crawled tweets: \", num_crled_tweets)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".................Crawling......................\n",
      "Start idx: 0, end idx: 138817\n",
      ".................\n",
      "508503520845107200\t2014-09-07 06:34:21\t'#3tking Flood Waters Submerge Swathes of Kashmir; 120 Dead http://t.co/APxoKUbK1F'\n",
      "\n",
      "508503530420723712\t2014-09-07 06:34:23\t'FYI. There are worse floods in Orissa. Please look there as well my dear media vultures'\n",
      "\n",
      ".................\n",
      "508503520845107200\t2014-09-07 06:34:21\t'#3tking Flood Waters Submerge Swathes of Kashmir; 120 Dead http://t.co/APxoKUbK1F'\n",
      "\n",
      "508503530420723712\t2014-09-07 06:34:23\t'FYI. There are worse floods in Orissa. Please look there as well my dear media vultures'\n",
      "\n",
      ".................\n",
      "508503530420723712\t2014-09-07 06:34:23\t'FYI. There are worse floods in Orissa. Please look there as well my dear media vultures'\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-2f3eadf155c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# data_crawler(files[1], \"/home/nguyen/tweet_interpret_sum/datasets/unlabeled_data/2014_typhoon_hagupit_en/crawled_data/20141202-1140-PeterMosur-_20141203_vol-1.json.csv\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m data_crawler(files[1], \"/home/nguyen/tweet_interpret_sum/datasets/unlabeled_data/2014_pakistan_floods_en/crawled_data/2014-09-pakistan_floods_20140907_vol-1.json.csv\", start_tweet='', \n\u001b[0;32m----> 5\u001b[0;31m             error_file = \"/home/nguyen/tweet_interpret_sum/datasets/unlabeled_data/2014_pakistan_floods_en/crawled_data/2014-09-pakistan_floods_20140907_vol-1.error.txt\")\n\u001b[0m",
      "\u001b[0;32m<ipython-input-41-7485e5f4a146>\u001b[0m in \u001b[0;36mdata_crawler\u001b[0;34m(corpusFile, output_file, start_tweet, error_file, winSize)\u001b[0m\n\u001b[1;32m     37\u001b[0m                     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}\\t{}\\t{}\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreation_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msleepTime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             \u001b[0mnum_crled_tweets\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\".................Crawling......................\")\n",
    "# data_crawler(files[0], \"/home/nguyen/tweet_interpret_sum/datasets/unlabeled_data/2015_nepal_earthquake_en/crawled_data/150425104337_nepal_earthquake_20150425_vol-1.json.csv\")\n",
    "# data_crawler(files[1], \"/home/nguyen/tweet_interpret_sum/datasets/unlabeled_data/2014_typhoon_hagupit_en/crawled_data/20141202-1140-PeterMosur-_20141203_vol-1.json.csv\")\n",
    "data_crawler(files[1], \"/home/nguyen/tweet_interpret_sum/datasets/unlabeled_data/2014_pakistan_floods_en/crawled_data/2014-09-pakistan_floods_20140907_vol-1.json.csv\", start_tweet='', \n",
    "            error_file = \"/home/nguyen/tweet_interpret_sum/datasets/unlabeled_data/2014_pakistan_floods_en/crawled_data/2014-09-pakistan_floods_20140907_vol-1.error.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
