{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tweepy\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import csv\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# student account\n",
    "consumer_key = 'uOLcWq6subGIyYVpc2tLsZhT8'\n",
    "consumer_key_secret = '9fcReWPgPa0Rk2gRndOGuPlRtyfHysNjrttqzysxhFdeh8H6pA'\n",
    "access_token = '1323575376669515776-zwb8gu1WHH69yW0Je0pIxB20oRwkN0'\n",
    "access_token_secret = 'E98yzbiL8v3JehoWlfLYRchTqu7rjPXg31q2nzwpRdXZs'\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_key_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# research account\n",
    "# consumer_key = 'Y2wLR0x8hp0dZt9wZuN9QZXa4'\n",
    "# consumer_key_secret = '9JjTy7VFdthVf0zilNiIuDF5M5NUckXC7a74W9oTu2i9EAdvxZ'\n",
    "# access_token = '1323575376669515776-8D35EbpdHORxqIHISGymFn1jGiOgvA'\n",
    "# access_token_secret = 'yuCHCv1ZPg4bOfQe73sg3jYWcOrRvK6TGPW3IH3WA4nEE'\n",
    "\n",
    "# auth = tweepy.OAuthHandler(consumer_key, consumer_key_secret)\n",
    "# auth.set_access_token(access_token, access_token_secret)\n",
    "# api = tweepy.API(auth,proxy=\"127.0.0.1:8083\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en\n",
      "The status 1266978261701210112 is posted by geeksforgeeks\n",
      "This status says : \n",
      "\n",
      "'Avoid errors, not client calls\\n.\\nGeeks, Keep this going...\\n.\\n#sundayvibes #programming #programmingmemes #coding https://t.co/JkA5iStofZ'\n",
      "\n",
      "en\n",
      "The status 1266735261012111360 is posted by geeksforgeeks\n",
      "This status says : \n",
      "\n",
      "'With the access to our Job Portal, find the jobs that are best for you &amp; experience happy placement journey....\\n.\\nL… https://t.co/mzMMFVzjMv'\n",
      "\n",
      "en\n",
      "The status 1266342841648898049 is posted by geeksforgeeks\n",
      "This status says : \n",
      "\n",
      "'My reaction to this Lockdown : \\n\\n\"Der Lagi Lekin... Maine Ab Hai Jeena Seekh Liya\" \\n\\nWhat\\'s your reaction to it?\\n.… https://t.co/nH9L0eewSr'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# list of status IDs to be fetched  \n",
    "id_ = [1266978261701210112, 1266735261012111360, 1266342841648898049] \n",
    "  \n",
    "# fetching the statuses \n",
    "statuses = api.statuses_lookup(id_) \n",
    "  \n",
    "# printing the statuses \n",
    "for status in statuses: \n",
    "    print(status.lang)\n",
    "    print(\"The status \" + str(status.id) + \" is posted by \" + status.user.screen_name) \n",
    "    print(\"This status says : \\n\\n\" + repr(status.text), end = \"\\n\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet fetchedRT @ADB_HQ: NEWS: ADB grants $300M loan to help Philippines shift to new basic education system http://t.co/3BVfLpWnBO #K12program\n"
     ]
    }
   ],
   "source": [
    "tweetFetched = api.get_status('544815534475595777')\n",
    "print(\"Tweet fetched\" + tweetFetched.text)\n",
    "text = tweetFetched.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all input files of tweet ids\n",
    "def read_files(folder):\n",
    "    files = []\n",
    "    for dir, _, filenames in os.walk(folder):\n",
    "        while len(filenames) > 0:\n",
    "            f = filenames.pop()\n",
    "            if f.endswith(\".csv\"):\n",
    "                files.append(os.path.join(dir, f))\n",
    "    files.sort()         \n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nguyen/tweet_interpret_sum/datasets/unlabeled_data/2014_Philippines_Typhoon_Hagupit_en/tweet_ids/20141202-1140-PeterMosur-_20141203_vol-1.json.csv\n",
      "/home/nguyen/tweet_interpret_sum/datasets/unlabeled_data/2014_Philippines_Typhoon_Hagupit_en/tweet_ids/20141202-1140-PeterMosur-_20141204_vol-1.json.csv\n",
      "/home/nguyen/tweet_interpret_sum/datasets/unlabeled_data/2014_Philippines_Typhoon_Hagupit_en/tweet_ids/20141202-1140-PeterMosur-_20141205_vol-1.json.csv\n",
      "/home/nguyen/tweet_interpret_sum/datasets/unlabeled_data/2014_Philippines_Typhoon_Hagupit_en/tweet_ids/20141202-1140-PeterMosur-_20141206_vol-1.json.csv\n",
      "/home/nguyen/tweet_interpret_sum/datasets/unlabeled_data/2014_Philippines_Typhoon_Hagupit_en/tweet_ids/20141202-1140-PeterMosur-_20141207_vol-2.json.csv\n",
      "/home/nguyen/tweet_interpret_sum/datasets/unlabeled_data/2014_Philippines_Typhoon_Hagupit_en/tweet_ids/20141202-1140-PeterMosur-_20141207_vol-3.json.csv\n",
      "/home/nguyen/tweet_interpret_sum/datasets/unlabeled_data/2014_Philippines_Typhoon_Hagupit_en/tweet_ids/20141202-1140-PeterMosur-_20141208_vol-4.json.csv\n",
      "/home/nguyen/tweet_interpret_sum/datasets/unlabeled_data/2014_Philippines_Typhoon_Hagupit_en/tweet_ids/20141202-1140-PeterMosur-_20141209_vol-5.json.csv\n",
      "/home/nguyen/tweet_interpret_sum/datasets/unlabeled_data/2014_Philippines_Typhoon_Hagupit_en/tweet_ids/20141202-1140-PeterMosur-_20141214_vol-5.json.csv\n"
     ]
    }
   ],
   "source": [
    "files = read_files('/home/nguyen/tweet_interpret_sum/datasets/unlabeled_data/2014_Philippines_Typhoon_Hagupit_en/tweet_ids/')\n",
    "# files = read_files('/home/nguyen/tweet_interpret_sum/datasets/unlabeled_data/2014_pakistan_floods_en/tweet_ids/')\n",
    "# files = read_files('/home/nguyen/tweet_interpret_sum/datasets/unlabeled_data/2014_typhoon_hagupit_en/tweet_ids/')\n",
    "for file in files:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crawled_data = pd.read_csv('/home/nguyen/tweet_interpret_sum/datasets/unlabeled_data/2014_Philippines_Typhoon_Hagupit_en/crawled_data/20141202-1140-PeterMosur-_20141206_vol-1.json.csv', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>540003748760674305</td>\n",
       "      <td>2014-12-03 04:45:01</td>\n",
       "      <td>'RT @IOMCwC: CCCM #Guiuan doing preparation me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>540003786811793408</td>\n",
       "      <td>2014-12-03 04:45:10</td>\n",
       "      <td>'PAGASA: Typhoon #Hagupit is expected to enter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>540003796986773504</td>\n",
       "      <td>2014-12-03 04:45:12</td>\n",
       "      <td>'@aprilenerio @PanahonTV I vote for #Hagupit S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>540003870131232769</td>\n",
       "      <td>2014-12-03 04:45:30</td>\n",
       "      <td>'@TheVampsJames can I see your name on my foll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>540003980076527619</td>\n",
       "      <td>2014-12-03 04:45:56</td>\n",
       "      <td>'RT @iamMJae: @aprilenerio @PanahonTV I vote f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id           created_at  \\\n",
       "0  540003748760674305  2014-12-03 04:45:01   \n",
       "1  540003786811793408  2014-12-03 04:45:10   \n",
       "2  540003796986773504  2014-12-03 04:45:12   \n",
       "3  540003870131232769  2014-12-03 04:45:30   \n",
       "4  540003980076527619  2014-12-03 04:45:56   \n",
       "\n",
       "                                                text  \n",
       "0  'RT @IOMCwC: CCCM #Guiuan doing preparation me...  \n",
       "1  'PAGASA: Typhoon #Hagupit is expected to enter...  \n",
       "2  '@aprilenerio @PanahonTV I vote for #Hagupit S...  \n",
       "3  '@TheVampsJames can I see your name on my foll...  \n",
       "4  'RT @iamMJae: @aprilenerio @PanahonTV I vote f...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crawled_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_tweet=str(crawled_data.loc[crawled_data.shape[0]-1]['tweet_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= pd.read_csv(files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tweet_id'] = data['tweet_id'].apply(lambda x: str(x[1:-1]))\n",
    "tweet_ids = list(data['tweet_id'])\n",
    "last_tweet_idx = tweet_ids.index(last_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35469\n",
      "3716\n"
     ]
    }
   ],
   "source": [
    "print(len(tweet_ids))\n",
    "print(last_tweet_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'508610076861403136'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_ids[last_tweet_idx+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'508610063683317760'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetFetched = api.get_status(tweet_ids[last_tweet_idx])\n",
    "creation_time = tweetFetched.created_at\n",
    "text = tweetFetched.text\n",
    "tweet_id = tweetFetched.id_str\n",
    "lan = tweetFetched.lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/nguyen/tweet_interpret_sum/datasets/unlabeled_data/2014_Philippines_Typhoon_Hagupit_en/tweet_ids/20141202-1140-PeterMosur-_20141203_vol-1.json.csv',\n",
       " '/home/nguyen/tweet_interpret_sum/datasets/unlabeled_data/2014_Philippines_Typhoon_Hagupit_en/tweet_ids/20141202-1140-PeterMosur-_20141204_vol-1.json.csv',\n",
       " '/home/nguyen/tweet_interpret_sum/datasets/unlabeled_data/2014_Philippines_Typhoon_Hagupit_en/tweet_ids/20141202-1140-PeterMosur-_20141205_vol-1.json.csv',\n",
       " '/home/nguyen/tweet_interpret_sum/datasets/unlabeled_data/2014_Philippines_Typhoon_Hagupit_en/tweet_ids/20141202-1140-PeterMosur-_20141206_vol-1.json.csv',\n",
       " '/home/nguyen/tweet_interpret_sum/datasets/unlabeled_data/2014_Philippines_Typhoon_Hagupit_en/tweet_ids/20141202-1140-PeterMosur-_20141207_vol-2.json.csv',\n",
       " '/home/nguyen/tweet_interpret_sum/datasets/unlabeled_data/2014_Philippines_Typhoon_Hagupit_en/tweet_ids/20141202-1140-PeterMosur-_20141207_vol-3.json.csv',\n",
       " '/home/nguyen/tweet_interpret_sum/datasets/unlabeled_data/2014_Philippines_Typhoon_Hagupit_en/tweet_ids/20141202-1140-PeterMosur-_20141208_vol-4.json.csv',\n",
       " '/home/nguyen/tweet_interpret_sum/datasets/unlabeled_data/2014_Philippines_Typhoon_Hagupit_en/tweet_ids/20141202-1140-PeterMosur-_20141209_vol-5.json.csv',\n",
       " '/home/nguyen/tweet_interpret_sum/datasets/unlabeled_data/2014_Philippines_Typhoon_Hagupit_en/tweet_ids/20141202-1140-PeterMosur-_20141214_vol-5.json.csv']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nguyen/tweet_interpret_sum/datasets/unlabeled_data/2014_Philippines_Typhoon_Hagupit_en/tweet_ids/20141202-1140-PeterMosur-_20141206_vol-1.json.csv\n"
     ]
    }
   ],
   "source": [
    "file = files[3]\n",
    "print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_crawler(corpusFile, output_file, start_tweet='', error_file ='', winSize = 100):\n",
    "\n",
    "    counter = 0\n",
    "    tweet_ids = []\n",
    "    with open(output_file, \"w\") as f:\n",
    "        f.write(\"tweet_id\\tcreated_at\\ttext\\n\")\n",
    "\n",
    "    data = pd.read_csv(corpusFile)\n",
    "\n",
    "    data['tweet_id'] = data['tweet_id'].apply(lambda x: str(x[1:-1]))\n",
    "    tweet_ids = list(data['tweet_id'])\n",
    "    \n",
    "    start_idx = 0\n",
    "    end_idx = len(tweet_ids)\n",
    "    \n",
    "    if start_tweet!=\"\":\n",
    "        start_idx = tweet_ids.index(start_tweet)+1\n",
    "    print(\"Start idx: {}, end idx: {}\".format(start_idx, end_idx))\n",
    "    \n",
    "    sleepTime = 3\n",
    "    trainingDataSet = []\n",
    "    i = 0\n",
    "    num_crled_tweets = 0\n",
    "    idx = start_idx\n",
    "    while idx < end_idx:\n",
    "#         print(\".................\")\n",
    "        ids_ = tweet_ids[idx: idx+winSize]\n",
    "        if idx+winSize >=end_idx:\n",
    "            ids_ = tweet_ids[idx:end_idx]\n",
    "#         print(ids_)\n",
    "        idx+=winSize\n",
    "        i+=winSize\n",
    "        \n",
    "        try:\n",
    "#             tweetFetched = api.get_status(int(tweet_id))\n",
    "            statuses = api.statuses_lookup(ids_) \n",
    "            for tweet in statuses:\n",
    "                if tweet.lang !=\"en\":\n",
    "                    continue\n",
    "#                 print(\"{}\\t{}\\t{}\\n\".format(tweet.id, tweet.created_at, repr(tweet.text)))\n",
    "                with open(output_file, \"a\") as f:\n",
    "                    f.write(\"{}\\t{}\\t{}\\n\".format(tweet.id, tweet.created_at, repr(tweet.text)))\n",
    "                \n",
    "            time.sleep(sleepTime)\n",
    "            num_crled_tweets +=len(statuses)\n",
    "            if i%500 == 0:\n",
    "                print(\"i:{}, #cwled:{}, {}\".format(i, num_crled_tweets, ids_[-1]))\n",
    "            \n",
    "        except Exception as exp:\n",
    "            print(\"++Exp: \", i, ids_, exp)\n",
    "#             with open(error_file, \"w\") as f:\n",
    "#                 f.write(\"{},{},{}\\n\".format(i, ids_, exp))\n",
    "        \n",
    "    print(\"#######\")\n",
    "    print(\"Number of tweet ids: \", i)\n",
    "    print(\"Number of crawled tweets: \", num_crled_tweets)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".................Crawling......................\n",
      "Start idx: 0, end idx: 120617\n",
      "i:500, #cwled:307, 541098768955752448\n",
      "i:1000, #cwled:590, 541100473001185280\n",
      "i:1500, #cwled:857, 541101891271532545\n",
      "i:2000, #cwled:1154, 541103565256019971\n",
      "i:2500, #cwled:1456, 541105320920383488\n",
      "i:3000, #cwled:1749, 541106975715512322\n",
      "i:3500, #cwled:2049, 541108654544396289\n",
      "i:4000, #cwled:2371, 541110268118331392\n",
      "i:4500, #cwled:2708, 541111954140459008\n",
      "i:5000, #cwled:3007, 541113477755310080\n",
      "i:5500, #cwled:3301, 541115352977928192\n",
      "i:6000, #cwled:3595, 541117292776419328\n",
      "i:6500, #cwled:3891, 541119094699741184\n",
      "i:7000, #cwled:4160, 541120545039724545\n",
      "i:7500, #cwled:4412, 541121547000221696\n",
      "i:8000, #cwled:4609, 541122089130811392\n",
      "i:8500, #cwled:4854, 541122852850630657\n",
      "i:9000, #cwled:5095, 541123805943308288\n",
      "i:9500, #cwled:5335, 541124784587669504\n",
      "i:10000, #cwled:5617, 541125788364722176\n",
      "i:10500, #cwled:5855, 541126905244549120\n",
      "i:11000, #cwled:6089, 541128148796993537\n",
      "i:11500, #cwled:6343, 541129309369298944\n",
      "i:12000, #cwled:6619, 541130599285538816\n",
      "i:12500, #cwled:6892, 541131972446527488\n",
      "i:13000, #cwled:7171, 541133329920049152\n",
      "i:13500, #cwled:7444, 541134788845789185\n",
      "i:14000, #cwled:7707, 541136186060730370\n",
      "i:14500, #cwled:7973, 541137681648852992\n",
      "i:15000, #cwled:8239, 541139090456866816\n",
      "i:15500, #cwled:8526, 541140640030294016\n",
      "i:16000, #cwled:8793, 541142181021360129\n",
      "i:16500, #cwled:9073, 541143927043022848\n",
      "i:17000, #cwled:9345, 541145614914822144\n",
      "i:17500, #cwled:9633, 541147165067071488\n",
      "i:18000, #cwled:9918, 541148865949614080\n",
      "i:18500, #cwled:10196, 541150573920538624\n",
      "i:19000, #cwled:10451, 541151986033254402\n",
      "i:19500, #cwled:10732, 541153318849224704\n",
      "i:20000, #cwled:11032, 541154949212549120\n",
      "i:20500, #cwled:11322, 541156282372075520\n",
      "i:21000, #cwled:11624, 541157575153434624\n",
      "i:21500, #cwled:11921, 541158675318980610\n",
      "i:22000, #cwled:12227, 541159600863449089\n",
      "i:22500, #cwled:12513, 541161012742082560\n",
      "i:23000, #cwled:12777, 541162329727959040\n",
      "i:23500, #cwled:13073, 541163743258759168\n",
      "i:24000, #cwled:13333, 541165115924832256\n",
      "i:24500, #cwled:13612, 541166237607149568\n",
      "i:25000, #cwled:13872, 541167390080651264\n",
      "i:25500, #cwled:14115, 541168514602516480\n",
      "i:26000, #cwled:14371, 541169234227634176\n",
      "i:26500, #cwled:14651, 541170350654898179\n"
     ]
    }
   ],
   "source": [
    "print(\".................Crawling......................\")\n",
    "# data_crawler(files[0], \"/home/nguyen/tweet_interpret_sum/datasets/unlabeled_data/2015_nepal_earthquake_en/crawled_data/150425104337_nepal_earthquake_20150425_vol-1.json.csv\")\n",
    "data_crawler(files[3], \"/home/nguyen/tweet_interpret_sum/datasets/unlabeled_data/2014_Philippines_Typhoon_Hagupit_en/crawled_data/20141202-1140-PeterMosur-_20141206_vol-1.json.csv\")\n",
    "# data_crawler(files[1], \"/home/nguyen/tweet_interpret_sum/datasets/unlabeled_data/2014_pakistan_floods_en/crawled_data/2014-09-pakistan_floods_20140907_vol-1.json.csv\", start_tweet=last_tweet, \n",
    "#             error_file = \"/home/nguyen/tweet_interpret_sum/datasets/unlabeled_data/2014_pakistan_floods_en/crawled_data/2014-09-pakistan_floods_20140907_vol-1.error.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
