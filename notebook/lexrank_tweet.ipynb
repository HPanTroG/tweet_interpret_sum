{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarization\n",
    "Lexrank with tfidf & LSH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "nlp_path = os.path.abspath('../')\n",
    "if nlp_path not in sys.path:\n",
    "    sys.path.insert(0, nlp_path)\n",
    "from utils.tweet_preprocessing import tokenizeRawTweetText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ehoang/miniconda3/envs/py37/lib/python3.7/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "[nltk_data] Downloading package stopwords to /home/ehoang/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy.linalg import norm\n",
    "from fast_pagerank import pagerank\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from itertools import combinations\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from utils.lsh import LSH\n",
    "from utils.fast_lexrank import Lexrank\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import pickle\n",
    "import time\n",
    "import string\n",
    "import re\n",
    "import emoji, string\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"/home/ehoang/hnt/tweet_interpret_sum/datasets/unlabeled_data/2015_nepal_earthquake_en/prediction_data/\"\n",
    "date = \"20150427\"\n",
    "output = \"/home/ehoang/hnt/tweet_interpret_sum/datasets/sum_groundtruth/2015_nepal_earthquake/\"+date+\"/\"\n",
    "# folder = \"/home/ehoang/hnt/tweet_interpret_sum/datasets/unlabeled_data/2015_nepal_earthquake_en/prediction_data\"\n",
    "# date = \"20150425\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_files(folder, date, is_return_files = False, delimiter = \"\\t\"):\n",
    "    if is_return_files ==True:\n",
    "        data = []\n",
    "    else:\n",
    "        data = pd.DataFrame()\n",
    "    if os.path.isfile(folder):\n",
    "        files = [folder]\n",
    "    else:\n",
    "        for dir, _, filenames in os.walk(folder):\n",
    "            while len(filenames) > 0:\n",
    "                file = filenames.pop()\n",
    "                if file.endswith(\".csv\") and date in str(file):\n",
    "                    if is_return_files ==True:\n",
    "                        data.append(os.path.join(dir, file))\n",
    "                    else:\n",
    "                        temp = pd.read_csv(os.path.join(dir, file), delimiter=delimiter)\n",
    "                        data = pd.concat([data, temp])\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(352648, 6)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read all tweets of the days\n",
    "data_all = extract_files(folder, date, is_return_files = False)\n",
    "data_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(352648, 3)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_all = data_all[['tweet_id', 'tweet_text', 'predicted_label']]\n",
    "data_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data before removing duplicates (352648, 3)\n",
      "Data after removing duplicates (155828, 3)\n"
     ]
    }
   ],
   "source": [
    "# remove duplicates\n",
    "print(\"Data before removing duplicates\", data_all.shape)\n",
    "data_all.drop_duplicates(subset=['tweet_text'], inplace=True, keep='first')\n",
    "print(\"Data after removing duplicates\", data_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#preprocess text\n",
    "punctuation = \"!\\\"$%&'()*+,-./:;<=>?@[\\]^_`{|}~”…’”—→\"\n",
    "data_all['tweet_text'] = data_all['tweet_text'].apply(lambda x: str(re.sub(\"^'|'$\", \"\", x)))\n",
    "\n",
    "data_all['prepro_text'] = data_all['tweet_text'].apply(lambda x: tokenizeRawTweetText(x))\n",
    "data_all['prepro_text'] = data_all['prepro_text'].apply(lambda x: x.replace(\"\\\\n\", \" \"))\n",
    "lemmatizer  = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "data_all['prepro_text'] = data_all['prepro_text'].apply(lambda x: ' '.join([lemmatizer.lemmatize(y) for y in x.split(\" \")]))\n",
    "data_all['prepro_text'] = data_all['prepro_text'].apply(lambda x: ' '.join([y for y in x.split(\" \") if y not in stop_words]))\n",
    "data_all['prepro_text'] = data_all['prepro_text'].apply(lambda x: x.translate(str.maketrans('', '', punctuation)).strip())\n",
    "data_all['prepro_text'] = data_all['prepro_text'].apply(lambda x: ''.join(c for c in x if c not in emoji.UNICODE_EMOJI).strip())\n",
    "data_all['prepro_text'] = data_all['prepro_text'].apply(lambda x: re.sub(\" +\", \" \", x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>predicted_label</th>\n",
       "      <th>prepro_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>592763070298066944</td>\n",
       "      <td>RT @SpokespersonMoD: #NepalEarthquake Casualty...</td>\n",
       "      <td>affected_people_and_evacuations</td>\n",
       "      <td>#nepalearthquake casualty evacuation fine exam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>592763069945765888</td>\n",
       "      <td>RT @Harry_Styles: To help those affected by th...</td>\n",
       "      <td>not_related_or_irrelevant</td>\n",
       "      <td>help affected devastating earthquake nepal tex...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id                                         tweet_text  \\\n",
       "0  592763070298066944  RT @SpokespersonMoD: #NepalEarthquake Casualty...   \n",
       "1  592763069945765888  RT @Harry_Styles: To help those affected by th...   \n",
       "\n",
       "                   predicted_label  \\\n",
       "0  affected_people_and_evacuations   \n",
       "1        not_related_or_irrelevant   \n",
       "\n",
       "                                         prepro_text  \n",
       "0  #nepalearthquake casualty evacuation fine exam...  \n",
       "1  help affected devastating earthquake nepal tex...  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_all.head(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data before removing duplicates (155828, 4)\n",
      "Data after removing duplicates (102918, 4)\n"
     ]
    }
   ],
   "source": [
    "# remove duplicates\n",
    "print(\"Data before removing duplicates\", data_all.shape)\n",
    "data_all.drop_duplicates(subset=['prepro_text'], inplace=True, keep='first')\n",
    "print(\"Data after removing duplicates\", data_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = data_all.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102918, 4)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rescue_volunteering_and_donation_effort                22977\n",
      "other_useful_information                               12689\n",
      "not_related_or_irrelevant                              42385\n",
      "infrastructure_and_utilities_damage                     3959\n",
      "injured_or_dead_people                                 10288\n",
      "affected_people_and_evacuations                        10620\n"
     ]
    }
   ],
   "source": [
    "# extract data of the categories\n",
    "for cat in set(data_all['predicted_label']):\n",
    "    print(\"%-40s%20d\"%(cat, data_all[data_all['predicted_label'] == cat].shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22977, 4)\n"
     ]
    }
   ],
   "source": [
    "current_cat = \"rescue_volunteering_and_donation_effort\"\n",
    "data = data_all[data_all['predicted_label'] == current_cat]\n",
    "print(data.shape)\n",
    "data = data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Lexrank + lsh + tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22977, 16702)\n"
     ]
    }
   ],
   "source": [
    "#extract tfidf vector\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidfData = tfidf.fit_transform(data['prepro_text'])\n",
    "print(tfidfData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22977,)\n"
     ]
    }
   ],
   "source": [
    "lsh_tfidf = LSH(tfidfData)\n",
    "lsh_tfidf.train(num_bits = 6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63}"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsh_tfidf.model['bin_indices']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#buckets: 64\n",
      ".......Buck: 0, vec: (15831, 16702)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lex_tfidf = Lexrank(tfidfData, lsh_tfidf)\n",
    "lex_tfidf.build_graph(search_radius = 3, cosine_sim = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex_tfidf.train(lexrank_iter = 100, damping_factor = 0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentIds = lex_tfidf.extract_summary(n_sents = 200, cosine_thres=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Id\", \"#adjacentEdges\", \"lexrank\")\n",
    "for i, idx in enumerate(sentIds):\n",
    "    print(i, len(lex_tfidf.graph[idx]), lex_tfidf.scores[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# nearby_bin = 0\n",
    "with open(output+current_cat+\".txt\", \"w\") as f:\n",
    "    for i, idx in enumerate(sentIds):\n",
    "        print(\"{}.\\t{}\\t{}\".format(i, data.iloc[idx]['tweet_id'], data.iloc[idx]['tweet_text']))\n",
    "        f.write(\"{}.\\t{}\\t{}\\n\".format(i, data.iloc[idx]['tweet_id'], data.iloc[idx]['tweet_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # nearby_bin = 0\n",
    "# for i, idx in enumerate(sentIds):\n",
    "#     print(i, data.iloc[idx]['tweet_id'], data.iloc[idx]['prepro_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
